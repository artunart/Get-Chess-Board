{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f763eb",
   "metadata": {},
   "source": [
    "<h1>Part 3a - Chessboard Detection: Train + Predict with Model Results</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa7e0a",
   "metadata": {},
   "source": [
    "<h2>Board_Detection Object</h2>\n",
    "\n",
    " A labeler, trainer and a detector for screenshots. Labeler is implemented in gcb_utils while trainer and detector are interfaces into PyTorch implementation of Yolov5 (https://github.com/ultralytics/yolov5, see https://pjreddie.com/media/files/papers/yolo.pdf for Yolo.) Main methods are update_labels, train and predict.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e173a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from board_detection import Board_Detection\n",
    "import gcb_utils.gcb_utils as gcb_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466af685",
   "metadata": {},
   "source": [
    "<h2>Initialize the Model</h2>\n",
    "\n",
    "Note that we're not specifying model weights, yet. It's an optional parameter at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd41b310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model weight file specified. You can train a new model using Board_Detection.train.\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'data/model/board-final_weight/'\n",
    "\n",
    "board_detector = Board_Detection(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2147c",
   "metadata": {},
   "source": [
    "<h2>Update Labels - Board_Detection.update_labels()</h2>\n",
    "\n",
    "Although demonstrated in Part 2a, I am including the update_labels() function of Board_Detection here for completeness.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf4ce9c-2f5e-4ef3-bf62-076a4b02e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please uncomment the below for documentation\n",
    "#print(help(board_detector.update_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb42f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "screenshot_data_path = 'data/raw/screenshots'\n",
    "screenshot_labels_fname = 'data/model/screenshot_boundboxes.csv'\n",
    "SCREENSHOT_LABEL_COLUMNS = ['fname', 'height_pxl','width_pxl','label','x_min_pxl','y_min_pxl'    ,'x_max_pxl','y_max_pxl', 'HumCheck-YN']\n",
    "update_fn = gcb_utils.screenshot_height_width_update\n",
    "\n",
    "board_detector.update_labels(screenshot_data_path, screenshot_labels_fname, SCREENSHOT_LABEL_COLUMNS, update_fn=update_fn, update_fn_kwargs={'screenshot_path':screenshot_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23ebc3",
   "metadata": {},
   "source": [
    "<h2>Training-i: 1-Epoch Run in the Notebook</h2>\n",
    "\n",
    "<li>The function .train() uses yolov5.detect() for training. (I've tried as much as I can to not modify the yolov5 directory, however please note the   __init__.py file I've inserted into the yolov5 directory) <br><br>\n",
    "\n",
    "<li>Following is a-1 epoch training of the dataset for demonstration purposes. Actual training took around 39 hours on a Macbook Air M1 where each epoch took approximately 4 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec70ded-2015-424c-8e14-6b8d8e651d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please uncomment the below for documentation\n",
    "#print(help(board_detector.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd58dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5/yolov5s.pt, cfg=, data=data/model/scr_data_cli.yaml, hyp=yolov5/data/hyps/hyp.scratch.yaml, epochs=1, batch_size=16, imgsz=1440, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=True, image_weights=False, device=cpu, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=data/model/board-train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache=True, cfg='', data='data/model/scr_data_cli.yaml', device='cpu', entity=None, epochs=1, evolve=None, exist_ok=False, freeze=0, hyp=PosixPath('yolov5/data/hyps/hyp.scratch.yaml'), image_weights=False, imgsz=1440, label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='exp', noautoanchor=False, nosave=False, noval=False, patience=100, project='data/model/board-train', quad=False, rect=False, resume=False, save_period=-1, single_cls=False, sync_bn=False, upload_dataset=False, weights='yolov5/yolov5s.pt', workers=8)\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v6.0-114-ga4207a2 torch 1.9.0 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir data/model/board-train', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è YOLOv5 is out of date by 27 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 üöÄ runs (RECOMMENDED)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 270 layers, 7022326 parameters, 7022326 gradients\n",
      "\n",
      "Transferred 343/349 items from yolov5/yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/raw/screenshots/train.cache' images and labels... 64 found, 0 missing, 0 empt\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB True): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:00<00:00, 625.81it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'data/raw/screenshots/validation.cache' images and labels... 18 found, 0 missing, 0 e\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB True): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00<00:00, 485.50it/s]\u001b[0m\n",
      "Plotting labels to data/model/board-train/exp/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.27 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Image sizes 1440 train, 1440 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mdata/model/board-train/exp\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "       0/0        0G    0.1162    0.1207         0        49      1440: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [04:19\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 all         18         24    0.00207       0.25    0.00115   0.000164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: NMS time limit 10.0s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.077 hours.\n",
      "\n",
      "Validating data/model/board-train/exp/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7012822 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from data/model/board-train/exp/weights/last.pt, 14.9MB\n",
      "Optimizer stripped from data/model/board-train/exp/weights/best.pt, 14.9MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: NMS time limit 10.0s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 all         18         24    0.00205       0.25    0.00114   0.000163\n",
      "Results saved to \u001b[1mdata/model/board-train/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "yaml_path_or_dict = 'data/model/scr_data_cli.yaml' #file pointing to inputs and class labels.\n",
    "init_weights_dir = 'yolov5'\n",
    "init_weights_fname = 'yolov5s.pt' #weights provided yolov5 pretrained on COCO\n",
    "train_project_dir = 'data/model/board-train'\n",
    "train_project_name = 'exp'\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "board_detector.train(yaml_path_or_dict, \n",
    "                     init_weights_dir, \n",
    "                     init_weights_fname, \n",
    "                     train_project_dir, \n",
    "                     train_project_name, \n",
    "                     epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d892d-3763-4516-94e1-cb82f6a5994b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Training-ii: Actual Training and Results:</h2>\n",
    "    \n",
    "<h3> Training parameters:</h3>\n",
    "\n",
    "The dataset contained 64 images for training and 18 for validation. <br>\n",
    "    \n",
    "I have run model on the command line at yolov5 root with: (the environment can be found at requirements.txt at the repo root)  <br>\n",
    "    python train.py --img 1440  --epochs 1500 --data ../data/model/scr_data.yaml --weights yolov5s.pt --cache --device cpu\n",
    "\n",
    "Going over the command line options used:\n",
    "    <li>--img 1440 -> size in pixels for the longer side of image\n",
    "    <li>--epochs 1500 -> Time cost of early termination was high - I picked a large number.\n",
    "    <li>--data -> indicates the yaml file created for the dataset (see Part 2a)\n",
    "    <li>--weights -> initial pretrained weights from which to start\n",
    "    <li>for any other options (e.g. patience, please refer to yolov5.train.py, or to a narrower extent, Board_Detection.train)\n",
    "\n",
    "\n",
    "        \n",
    "<h3> Results:</h3>\n",
    "Final results on an Apple MacBook Air (2020) M1 with 16GB of memory are as follows. Each epoch took approximately 4 minutes. Below, I show the best (498) and final (598) epochs. The final epoch is less than the indicated 1500 due to default patience parameter of 100. If an epoch survives as best after 100 steps, training is stopped.  \n",
    "    \n",
    "![](z_markdown_jpgs/Board-Best+FinalEpochs.png) <br>\n",
    "\n",
    "<li>Over the 18 images used for validation, mean Average Precision (mAP) at an Intersection Over Union (IOU) ratio of 0.5 was 0.987. Over a range of IOU's from 0.5 to 0.95 mAP stood at 0.953. Also, please note that precision is 0.96 and recall is >0.995 in the validation set. The precision-recall graph from yolov5 is presented below.<br> <br>\n",
    "    \n",
    "![](z_markdown_jpgs/BoardPR_curve.png) <br>\n",
    "    \n",
    "<li>Although, there is a chance that model could be overfit due to these high figures, unfortunately I did not have the chance to experiment with the model given the high time cost of training. The results from test images however, show that the fit is still functional out of the sample for the purpose of detecting chessboards. <br><br>\n",
    "    \n",
    "<li>Below I also present a validation image mosaic from yolov5 output.\n",
    " \n",
    " ![](z_markdown_jpgs/Boardval_batch0_labels.jpg) <br>\n",
    "    Validation Set - Labels <br><br>\n",
    "    \n",
    " ![](z_markdown_jpgs/Boardval_batch0_pred.jpg) <br>\n",
    "    Validation Set - Predictions <br><br>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95bb5d-f0ec-482c-8d13-4a73f44a5873",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Detection (.predict): Demonstration using Final Model and Test Images</h2>\n",
    " \n",
    ".predict() outputs board predictions of files in source_dir with bounding boxes using the yolov5.detect function. The function also returns a pandas DataFrame that contains any labels it found for the image set.\n",
    "\n",
    "Below, I will use the final model weights from the longer run and apply them on 10 test images which did not participate in training. Note that model_dir has to include hubconf.py. Function help is available below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6f7035-5c7a-4f9b-bcc3-08a6e9bd0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please uncomment the below for documentation\n",
    "#print(help(board_detector.predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "736e23e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v6.0-114-ga4207a2 torch 1.9.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7012822 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: --img-size (1440, 1440) must be multiple of max stride 32, updating to [1440, 1440]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/Screen Shot 2021-12-01 at 7.44.47 PM.jpg: 928x1440 4 Chessboards, Done. (0.435s)\n",
      "image 2/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/Screen Shot 2021-12-01 at 7.45.27 PM.jpg: 928x1440 1 Chessboard, Done. (0.407s)\n",
      "image 3/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/Screen Shot 2021-12-01 at 7.46.17 PM.jpg: 928x1440 3 Chessboards, Done. (0.408s)\n",
      "image 4/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/Screen Shot 2021-12-01 at 7.55.43 PM.jpg: 928x1440 2 Chessboards, Done. (0.409s)\n",
      "image 5/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/Screen Shot 2021-12-01 at 7.56.39 PM.jpg: 928x1440 1 Chessboard, Done. (0.407s)\n",
      "image 6/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/Screen Shot 2021-12-01 at 7.58.46 PM.jpg: 928x1440 1 Chessboard, Done. (0.411s)\n",
      "image 7/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/Screen Shot 2021-12-01 at 7.58.54 PM.jpg: 928x1440 1 Chessboard, Done. (0.411s)\n",
      "image 8/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/Screen Shot 2021-12-01 at 8.03.09 PM.jpg: 928x1440 3 Chessboards, Done. (0.408s)\n",
      "image 9/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/Screen Shot 2021-12-01 at 8.13.14 PM.jpg: 928x1440 1 Chessboard, Done. (0.412s)\n",
      "image 10/10 /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/raw/screenshots/test/chess_1.jpg: 928x1440 1 Chessboard, Done. (0.412s)\n",
      "Speed: 1.4ms pre-process, 412.0ms inference, 0.6ms NMS per image at shape (1, 3, 1440, 1440)\n",
      "Results saved to \u001b[1mdata/model/board-detect/det\u001b[0m\n",
      "10 labels saved to data/model/board-detect/det/labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolov5 detection results are saved in: /Users/artun/Desktop/Springboard/springboard/Get-Chess-Board/data/model/board-detect/det\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'data/model/board-final_weight'\n",
    "model_weight_fname = 'board_best.pt' \n",
    "source_dir = 'data/raw/screenshots/test'\n",
    "imgsz = (1440, 1440) #Square shape image size \n",
    "project_dir = 'data/model/board-detect'\n",
    "project_name = 'det'\n",
    "conf_thres = 0.10\n",
    "\n",
    "board_detector = Board_Detection(model_dir, model_weight_fname)\n",
    "prediction = board_detector.predict(model_dir, \n",
    "                                    model_weight_fname, \n",
    "                                    source_dir, \n",
    "                                    imgsz, \n",
    "                                    project_dir, \n",
    "                                    project_name,  \n",
    "                                    conf_thres, \n",
    "                                    save_txt = True, \n",
    "                                    save_conf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46f0634-b56a-4f93-a955-b4dc99f24af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "      <th>x_min_norm</th>\n",
       "      <th>y_min_norm</th>\n",
       "      <th>x_max_norm</th>\n",
       "      <th>y_max_norm</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Screen Shot 2021-12-01 at 7.44.47 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0.295139</td>\n",
       "      <td>0.480556</td>\n",
       "      <td>0.547222</td>\n",
       "      <td>0.858889</td>\n",
       "      <td>0.863039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Screen Shot 2021-12-01 at 7.44.47 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724306</td>\n",
       "      <td>0.673889</td>\n",
       "      <td>0.123611</td>\n",
       "      <td>0.078889</td>\n",
       "      <td>0.877888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Screen Shot 2021-12-01 at 7.44.47 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0.648264</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.071528</td>\n",
       "      <td>0.115556</td>\n",
       "      <td>0.934162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Screen Shot 2021-12-01 at 7.46.17 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747569</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.071528</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.932372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Screen Shot 2021-12-01 at 7.46.17 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0.502431</td>\n",
       "      <td>0.506111</td>\n",
       "      <td>0.536806</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.943299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Screen Shot 2021-12-01 at 8.03.09 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.052778</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.920412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Screen Shot 2021-12-01 at 8.03.09 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>0.496111</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.934135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Screen Shot 2021-12-01 at 7.55.43 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0.481597</td>\n",
       "      <td>0.850556</td>\n",
       "      <td>0.057639</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.918741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  fname label  x_min_norm  y_min_norm  \\\n",
       "0  Screen Shot 2021-12-01 at 7.44.47 PM     0    0.295139    0.480556   \n",
       "1  Screen Shot 2021-12-01 at 7.44.47 PM     0    0.724306    0.673889   \n",
       "2  Screen Shot 2021-12-01 at 7.44.47 PM     0    0.648264    0.830000   \n",
       "0  Screen Shot 2021-12-01 at 7.46.17 PM     0    0.747569    0.855556   \n",
       "1  Screen Shot 2021-12-01 at 7.46.17 PM     0    0.502431    0.506111   \n",
       "0  Screen Shot 2021-12-01 at 8.03.09 PM     0    0.395833    0.855000   \n",
       "1  Screen Shot 2021-12-01 at 8.03.09 PM     0    0.260417    0.496111   \n",
       "0  Screen Shot 2021-12-01 at 7.55.43 PM     0    0.481597    0.850556   \n",
       "\n",
       "   x_max_norm  y_max_norm  confidence  \n",
       "0    0.547222    0.858889    0.863039  \n",
       "1    0.123611    0.078889    0.877888  \n",
       "2    0.071528    0.115556    0.934162  \n",
       "0    0.071528    0.111111    0.932372  \n",
       "1    0.536806    0.861111    0.943299  \n",
       "0    0.052778    0.090000    0.920412  \n",
       "1    0.408333    0.630000    0.934135  \n",
       "0    0.057639    0.090000    0.918741  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is a pd.DataFrame\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f73f5",
   "metadata": {},
   "source": [
    "<h3>Board Detection Results</h3>\n",
    "\n",
    "Let's take a look at the board detection output by yolov5 on images not previously seen:\n",
    "\n",
    "  Some test outputs with correct classification:\n",
    "![](z_markdown_jpgs/Board_test_pred_ScreenShot2021-12-01at7.45.27PM.jpg) <br>\n",
    "![](z_markdown_jpgs/Board_test_pred_ScreenShot2021-12-01at7.56.39PM.jpg) <br><br><br>\n",
    "\n",
    "  However, although the model seems to perform well in test, not all classifications were correct. <br>\n",
    "  Here is an image with a False Positive (please note the partial chessboard and the smaller square with confidence values of 0.80):\n",
    "![](z_markdown_jpgs/Board_test_pred_ScreenShot2021-12-01at7.44.47PM.jpg) <br>\n",
    "\n",
    "  Here is also an image with a False Negative (please note the chessboard not labeled):\n",
    "![](z_markdown_jpgs/Board_test_pred_ScreenShot2021-12-01at7.55.43PM.jpg) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d59659-092a-47c3-95db-1b8beba434b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "220e10ff8ec8c0a2ee704b9ba1623fbaf79aa98a681a616ca665c089912d6d5d"
  },
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
